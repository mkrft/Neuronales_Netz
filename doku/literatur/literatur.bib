% TODO ==================================================
%
%   -> Entnahme Datum bei Online Quellen fehlt in der Arbeit?
%
%

@book{pattern_oriented_architecture,
        location = {Chichester},
        title = {Pattern-Oriented Software Architecture},
        isbn = {978-0-471-60695-6},
        shorttitle = {Pattern for Concurrent and Networked Objects},
        pagetotal = {},
        publisher = {John Wiley \& Sons, Ltd},
        author = {Schmidt, Douglas; Stal, Michael},
        year = {2000}
}

@book{discrete_event_sim_fishman,
        location = {New York},
        title = {Discrete-Event Simulation},
        isbn = {978-1-4419-2892-4},
        shorttitle = {Modeling, Programming and Analysis},
        pagetotal = {23},
        publisher = {Springer Science+Business Media New York},
        author = {Fishman, George S.},
        year = {2001}
}

@misc{nwankpa2018activation,
      title={Activation Functions: Comparison of trends in Practice and Research for Deep Learning}, 
      author={Chigozie Nwankpa and Winifred Ijomah and Anthony Gachagan and Stephen Marshall},
      year={2018},
      eprint={1811.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lederer2021activation,
      title={Activation Functions in Artificial Neural Networks: A Systematic Overview}, 
      author={Johannes Lederer},
      year={2021},
      eprint={2101.09957},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%Das bezieht sich auf ein Paper von Tomasz Szandała mit dem Titel Review and Comparison of Commonly Used Activation Functions for Deep Neural Networks
@article{2021ReviewandComparison,
       title={Bio-inspired Neurocomputing},
       author={Tomasz Szandala},
       ISBN={9789811554957},
       ISSN={1860-9503},
       url={http://dx.doi.org/10.1007/978-981-15-5495-7},
       DOI={10.1007/978-981-15-5495-7},
       journal={Studies in Computational Intelligence},
       publisher={Springer Singapore},
       year={2021} 
}

@misc{ImprovingNeuralNetworks,
  doi = {10.48550/ARXIV.1905.09574},
  url = {https://arxiv.org/abs/1905.09574},
  author = {Jung, Seongmun and Kwon, Oh Joon},
  keywords = {Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Improving Neural Networks by Adopting Amplifying and Attenuating Neurons},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ApproachforTrainingNeuralNetwork,
       author = {{Tsoi}, Nathan and {Candon}, Kate and {Milkessa}, Yofti and {V{\'a}zquez}, Marynel},
        title = "{An End-to-End Approach for Training Neural Network Binary Classifiers on Metrics Based on the Confusion Matrix}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = sep,
          eid = {arXiv:2009.01367},
        pages = {arXiv:2009.01367},
archivePrefix = {arXiv},
       eprint = {2009.01367},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200901367T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{polynomial_regression,
        title = {Modelling using Polynomial Regression},
        journal = {Procedia Engineering},
        volume = {48},
        pages = {500-506},
        year = {2012},
        note = {Modelling of Mechanical and Mechatronics Systems},
        issn = {1877-7058},
        doi = {https://doi.org/10.1016/j.proeng.2012.09.545},
        url = {https://www.sciencedirect.com/science/article/pii/S1877705812046085},
        author = {Eva Ostertagová}
    }


@online{techneuron,
        author = {o.A.},
        title ={NeuronModel deutsch.svg},
        url={https://commons.wikimedia.org/w/index.php?curid=674079},
        addendum = {(accessed: 30.01.2022)},
        keywords={Neuron}
}

@online{fast_f1,
        author = {o.A.},
        title ={Fast F1 Python API},
        url={https://theoehrly.github.io/Fast-F1/},
        addendum = {(accessed: 24.02.2022)},
        keywords={F1}
}

@online{math_poly_reg,
        author = {Weisstein, Eric W.},
        title = {Least Squares Fitting-Polynomial},
        publisher = {MathWorld},
        url = {https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html},
        year = {2022},
        appendum = {(accessed: 04.03.2022)}
}

@article{atari_deep_reinforcement_learning,
        doi = {10.48550/ARXIV.1312.5602},
        url = {https://arxiv.org/abs/1312.5602},
        author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
        keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Playing Atari with Deep Reinforcement Learning},
        publisher = {arXiv},
        year = {2013},
        copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{reinforcement_learning_study,
        doi = {10.48550/ARXIV.CS/9605103},
        url = {https://arxiv.org/abs/cs/9605103},
        author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
        keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
        title = {Reinforcement Learning: A Survey},
        publisher = {arXiv},
        year = {1996}
}

@article{dueling_network_architectures,
        author    = {Ziyu Wang and
                     Nando de Freitas and
                     Marc Lanctot},
        title     = {Dueling Network Architectures for Deep Reinforcement Learning},
        journal   = {CoRR},
        volume    = {abs/1511.06581},
        year      = {2015},
        url       = {http://arxiv.org/abs/1511.06581},
        eprinttype = {arXiv},
        eprint    = {1511.06581},
        timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
        biburl    = {https://dblp.org/rec/journals/corr/WangFL15.bib},
        bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{general_Q_learning,
        author = {Tambet Matiisen},
        title = {Demystifying Deep Reinforcement Learning},
        publisher = {University of Tartu},
        year = {2015},
        url = {https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/},
        appendum = {accessed: 23.04.2022}
    }
    
@misc{deep_reinforcement_learning,
  doi = {10.48550/ARXIV.1810.06339},
  url = {https://arxiv.org/abs/1810.06339},
  author = {Li, Yuxi},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Reinforcement Learning},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{goodfellow_deep_learning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

% Bild der Neuronenstruktur aus : https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg

% Pytorch
@incollection{PyTorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
} 


@misc{reinforcementlearningTutorial,
    title = {Reinforcement Learning: A Tutorial},
    author = {Mance Harmon and Stephanie Harmon},
    year = {1997},
    url = {https://apps.dtic.mil/sti/pdfs/ADA323194.pdf},
    appendum = {accessed: 19.05.2022}
}
@InProceedings{pmlr-v119-ayoub20a,
  title = 	 {Model-Based Reinforcement Learning with Value-Targeted Regression},
  author =       {Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {463--474},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/ayoub20a/ayoub20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/ayoub20a.html},
}

@misc{experience_replay_continual_learning,
  doi = {10.48550/ARXIV.1811.11682},
  url = {https://arxiv.org/abs/1811.11682},
  author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy P. and Wayne, Greg},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Experience Replay for Continual Learning},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{prioritized_experience_replay,
  doi = {10.48550/ARXIV.1511.05952},
  url = {https://arxiv.org/abs/1511.05952},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Prioritized Experience Replay},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{rainbow_dqn,
  doi = {10.48550/ARXIV.1710.02298},
  url = {https://arxiv.org/abs/1710.02298},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{double_q_learning,
  doi = {10.48550/ARXIV.1509.06461},
  url = {https://arxiv.org/abs/1509.06461},
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Reinforcement Learning with Double Q-learning},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@online{pytorchtensorflow,
        author = {Jesus Rogel-Salazar},
        title = {Tenorflow, Pytorch or Keras for Deep Learning},
        publisher = {Domino Data Lab},
        year = {2022},
        url = {https://blog.dominodatalab.com/tensorflow-pytorch-or-keras-for-deep-learning},
        appendum = {accessed: 08.06.2022}
}

@article{performancePytorchTensorflow,
author = {Florencio, Felipe and Silva, Thiago and Ordonez, Edward and Júnior, Methanias},
year = {2019},
month = {05},
pages = {},
title = {Performance Analysis of Deep Learning Libraries: TensorFlow and PyTorch},
volume = {15},
journal = {Journal of Computer Science},
doi = {10.3844/jcssp.2019.785.799}
}

@article{alpha_go, 
    year = {2017}, 
    title = {{Mastering the game of Go without human knowledge}}, 
    author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis}, 
    journal = {Nature}, 
    issn = {0028-0836}, 
    doi = {10.1038/nature24270}, 
    abstract = {{A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.}}, 
    pages = {354--359}, 
    number = {7676}, 
    volume = {550}
}

@article{dqn_implementation,
  doi = {10.48550/ARXIV.1711.07478},
  url = {https://arxiv.org/abs/1711.07478},
  author = {Roderick, Melrose and MacGlashan, James and Tellex, Stefanie},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Implementing the Deep Q-Network},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{adam_optimizer,
  doi = {10.48550/ARXIV.1412.6980},
  url = {https://arxiv.org/abs/1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{neuronbasics,
author = {Basheer, Imad and Hajmeer, M.N.},
year = {2001},
month = {01},
pages = {3-31},
title = {Artificial Neural Networks: Fundamentals, Computing, Design, and Application},
volume = {43},
journal = {Journal of microbiological methods},
doi = {10.1016/S0167-7012(00)00201-3}
}

@misc{neuronimage,
 url = {https://de.wikipedia.org/wiki/KüCnstliches_Neuron#/media/Datei:Neuron_(deutsch)-1.svg},
 title = {Schematische Darstellung einer Nervenzelle},
 appendum = {accessed: 28.12.2021},
 author={o.V.},
 year = {2006}
}

@article{signalconduction,
author = {Seidl, Armin},
year = {2013},
month = {06},
pages = {},
title = {Regulation of Conduction Time along Axons},
volume = {276},
journal = {Neuroscience},
doi = {10.1016/j.neuroscience.2013.06.047}
}

@book{introductiontoneuralnetworks,
author = {Gurney, Kevin},
title = {An Introduction to Neural Networks},
year = {1997},
isbn = {1857286731},
publisher = {Taylor &amp; Francis, Inc.},
address = {USA},
abstract = {From the Publisher:An Introduction to Nueral Networks will be warmly welcomed by a wide readership seeking an authoritative treatment of this key subject without an intimidating level of mathematics in the presentation.}
}

@misc{aineuron,
url={https://de.wikipedia.org/wiki/Künstliches_Neuron#/media/Datei:ArtificialNeuronModel_deutsch.png},
author={Christlb},
title={Darstellung eines künstlichen Neurons mit seinen Elementen},
year ={2005},
appendum = {accessed: 29.12.2021}
}