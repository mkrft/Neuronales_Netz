\section*{Abstract}
In our modern world, artificial intelligence continues to proliferate in a wide variety of fields. This process is particularly fueled by the increasing computing power available. Nevertheless, artificial intelligence lacks the necessary flexibility and creativity to solve particular problems. Accordingly, the goal of this thesis was to train an artificial intelligence for a problem that represents a borderline case between pure evaluation of data and human creativity. The decision of the racing strategy in motorsports.
More concretely, the AI is to learn how to determine an optimal racing strategy in a simulated Formula 1 environment. Here, the AI model is to determine the optimal time for a pit stop as well as the type of tire change based on the race events.\\\\
For this purpose, models were developed on the basis of real race data that abstract and formalize a wide variety of race strategy influences and make them computable for use in a specially developed simulation environment. Thus, using polynomial regression, models were developed that represent the influencing factor of tires in terms of lap speed and tire wear. Furthermore, formal abstractions were created for individual factors such as vehicle and driver performance. In addition, a formal, computable abstraction of the interactions of the race participants in the form of position battles and overtaking maneuvers was made.\\
Based on the created models, a simulator was implemented, which simulates a complete Formula 1 race in the form of discrete steps and serves as a training environment for the developed AI model.\
Using the simulation environment, an AI model suitable for determining the optimal racing strategy was developed. Due to the nature of the system, which does not allow for supervised learning, it proved to be appropriate to implement the principle of reinforcement learning. Here, the AI model decides for or against a pit stop per discrete simulation step based on current race state parameters and acts on the future states of the simulator through this action and receives a state-based reward accordingly. The Deep-Q learning algorithm was used as the concrete learning procedure for this purpose.\\
When looking at the results provided by the trained AI model within the simulated racing environment, it can be seen that this identifies a valid and potentially optimal racing strategy that provides good results within the abstracted racing environment.
It should be noted, however, that due to the abstraction of the simulator, the corresponding racing strategy allows only limited conclusions to be drawn about the applicability of the determined strategy in reality.

\newpage