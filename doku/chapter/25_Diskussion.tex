\section{Diskussion}\label{sec:discussion}
% Keine wirkliche Auswertung der Daten für die Wechselwirkungsmodelle
% Überholen potentiell zu einfach und nicht repräsentativ sieht aber in den Auswertungen bisher sehr gut aus
% Eintönige Strategie Wahl! -> zu einfaches Modell führt zu einfachem optimierten Fall -> keine Varianz
Durch die Auswertung des Verhaltens der KI in den verschiedenen Szenarien in \ref{sec:ai_performance} konnten Rückschlüsse auf die Gültigkeit der eingesetzten Modelle im Simulator geschlossen werden. So konnten besonders der Reifenverschleiß, die Leistungsparemeter eines einzelnen Fahrzeuges und die Wechselwirkungen der Fahrzeuge untereinander verifiziert und beobachtet werden.\\
Einschränkend ist hierbei aber zu erwähnen, dass bei der Untersuchung der Reifen die Mischungen nicht bis auf die offiziellen Kategorien C1-C5 aufgelöst wurden. Dies ist durch den Umstand der Datenlage gegeben, welche ebenfalls eine Vereinfachung auf Soft, Medium und Hard vornimmt. Der Einfluss dieser Abstraktion wird als sehr gering eingestuft, muss aber für eine potentielle Weiterführung oder einen Live-Einsatz beachtet werden.\\
Weiter ist zu erwähnen, dass die Generierung der Wechselwirkungsmodelle vereinfacht aus der Datengrundlage abgeleitet wurden. Dies ist durch den Umstand gegeben, dass beispielsweise der zusätzliche Reifenverschleiß nicht aus den Daten ersichtlich ist und somit nur im Rahmen der bekannten Rennen geschätzt werden konnte. Entsprechend verhält es sich mit der kalkulierten Wahrscheinlichkeit für das Gelingen eines Überholmanövers, welches so gewählt werden musste, dass das Überholen nicht zu leicht fällt, aber auch nicht unmöglich ist.
\\\\
%KI Dinge, der dem was einfällt kann hierzu gerne noch was schreiben
Im Kontext der KI - Entwicklung lässt sich festhalten, dass an verschiedenen Stellen Designentscheidungen getroffen wurden, die lediglich auf Heuristiken und keinerlei objektiven Metriken basieren. Dies gilt sowohl für die Hyperparameter und hidden-layer Größen des neuronalen Netzes als auch für elementare Bestandteile des finalen DQN-Lernprozesses wie der Belohnungsfunktion. Somit sind die im Rahmen der Arbeit erzielten Ergebnisse mit großer Sicherheit mit mehr Zeitaufwand noch zu verbessern. Ebenso ist es durch die instabile Natur der Derivate des DQN-Algorithmus nicht möglich, eine verifizierbar optimale Strategie zu entwickeln. Aufgrund des großen Einflusses der Rennstrategie im realen Geschehen ist die Ableitung dieser aus einer potentiell unsicheren Quelle ein Risiko. Demnach würde das Entwickeln eines verifizierbaren, deterministischen Ansatzes in diesem Kontext eine überlegene Lösung bieten. Insgesamt müsste die initial getroffene Technologieentscheidung für das modellfreie Reinforcement-Learning in Form des DQN für eine Optimierung der Ergebnisse neu untersucht werden, da speziell hinsichtlich des erarbeiteten Simulators Vorteile für den Lernprozess des Agenten aus der Verfügbarkeit einer vereinfachten Simulation der Umgebung zur Laufzeit erzielt werden könnten.
\\\\
In Bezug auf die erforschten und erlernten Strategien der KI sind die erarbeiteten Ergebnisse durch die starke Abstraktion des Simulators sehr eintönig. Dies bedeutet, wie aus der Untersuchung der diversen Szenarien in Kapitel \ref{sec:ai_performance} hervorgeht, dass, wenn die KI einmal die optimale Strategie lernt, sie diese auch in allen Fällen mit minimalen Abweichungen anwendet. Dieser Umstand ist in vollem Umfang der Einfachheit des Simulators geschuldet, da in diesem einfachen Rahmen die statistisch optimale Strategie gleichbleibend ist. Das bedeutet, dass erst mit einer Erweiterung des Simulators um zusätzliche Faktoren und Einflüsse wie Wetter, Unfälle und Beschädigungen das Renngeschehen eine solche Komplexität erreicht, sodass es mehrere optimale Rennstrategien gibt. Dies würde nach der entsprechenden Erweiterung des Simulators einen umfangreicheren und langwierigen Trainingsprozess bedeuten, aber dafür die Anwendbarkeit und Varianz der gefundenen Strategien erhöhen.